---
title: "part I - Keras, Tensorflow and MLP"
author: "Micha≈Ç Maj"
output: html_notebook
---

Before we start building neural networks using Keras let's see how R communicates with python.

```{r python_config}
library(keras)
library(reticulate)
library(tidyverse)
library(gridExtra)
# library(deepviz)
# install_keras() Keras installation form R
py_config() # Current config info
# Using commands below you can set up correct python path/env:
# use_python("/usr/local/bin/python")
# use_virtualenv("~/myenv")
# use_condaenv("myenv")
# You can also use RETICULATE_PYTHON system variable
Sys.getenv("RETICULATE_PYTHON")
# readLines(".Rprofile")
```

```{r reticulate_example}
np <- import("numpy")
np$max(c(4, 7, 2))
```

In Keras we can create models in two different ways:
- build sequential model - we're stacking new layers on top of previous ones. We can't use multiple inputs and outputs.
- using functional API - allows us to use multiple inputs and outputs.

We will start with sequential model. We have to start with a model initialization:

```{r sequential_model}
load("data/boston.RData")
# Check shape of the data
boston_train_X %>% dim() # Two dim tensor
boston_train_Y %>% dim() # One dim tensor

boston_model <- keras_model_sequential()
```

In the next step we can add some layers (note that we don't have to reassign the model with `<-`):

```{r add_layer}
boston_model %>% layer_dense(units = 16, # Number of neurons in the layer
                             activation = "tanh", # Activation function
                             input_shape = c(13)) # Nr of predictors. always in first layer!
boston_model
```

Why do we have 224 params ?

```{r nr_of_params}
13 * 16 + 16
```

After adding hidden layer we can add output layer:

```{r add_output_layer}
boston_model %>%
  layer_dense(units = 1,
              activation = "linear")
boston_model
```

We can now configure model for training. We will use SGD as optimizer, MSE as loss function and add MAE as additional metric.

```{r model_compilation}
boston_model %>% compile(
  optimizer = "sgd",
  loss = "mse",
  metrics = c("mae")
)
```

We are ready to train our first neural network:

```{r model_training}
history <- boston_model %>%
  fit(x = boston_train_X,
      y = boston_train_Y,
      validation_split = 0.2, # 20% of the data for validation
      epochs = 5, # Number of "loops" over whole dataset
      batch_size = 30, # Sample size for one run of SGD
      verbose = 1)
```

We can now evaluate trained model on the test dataset:

```{r model_evaluation}
boston_model %>%
  evaluate(boston_test_X, boston_test_Y)
```

And calculate predictions:

```{r prediction}
boston_predictions <- boston_model %>% predict(boston_test_X)
head(boston_predictions)
```

In the end we can save our model on hard drive:

```{r save_model}
if (!dir.exists("models")) dir.create("models")
save_model_hdf5(boston_model, "models/boston_model.hdf5")
```

We know now how to use MLP for regression tasks, let's check how to use it for classification problems. We will start with binary classification:

```{r load_bin_class_data}
load("data/bin_class.RData")
ggplot(bin_class_data, aes(x, y, color = factor(class))) + theme_bw() + geom_point()
```

We have to transform the data into tensors:

```{r data_to_tensors}
ind <- sample(1:nrow(bin_class_data), 0.8*nrow(bin_class_data))
bin_class_train_X <- bin_class_data[ind, c("x", "y")] %>% as.matrix()
bin_class_train_Y <- bin_class_data[ind, "class", drop = TRUE]
bin_class_test_X <- bin_class_data[-ind, c("x", "y")] %>% as.matrix()
bin_class_test_Y <- bin_class_data[-ind, "class", drop = TRUE]
```

Let's create a few simple models:

```{r bin_model}
# TASK: Create a sequential model with:
# a) one hidden layer b) two hidden layers
# Check variants with 2, 4 and 8 units in first layer
# Check variants with relu, tanh, sigmoid activations
# Add l1/l2 regularization using 'activity_regularizer' argument and regularizer_l1/2() functions
# Compile model with binary crossentropy as loss and SGD optimizer
# Fit the model using 10% of the data for validation, 100 epochs and batch_size of 100. Save runs to 'history' object. Use 'verbose' = 0.
# Evaluate model on the test data

history$metrics$epoch <- 1:100 # Epochs
history_df <- as.data.frame(history$metrics)
ggplot(history_df, aes(epoch, acc)) + theme_bw() + geom_line(color = "red") + geom_line(aes(y = val_acc), color = "blue")
```

```{r bin_model_plot}
predictions <- bin_model %>% predict(square_data) %>% cbind(square_data) %>%
  as.data.frame() %>% set_names(c("proba", "x", "y"))
ggplot(predictions, aes(x, y)) + theme_bw() + geom_raster(aes(fill = proba)) + geom_contour(colour = "white", aes(z = proba)) + scale_fill_gradient(low = "red", high = "blue") + geom_point(data = bin_class_data, aes(x, y, color = factor(class)))
```

In a similar way we can build sequential model for multi-class classification problem:

```{r fashion_mnist}
load("data/fashion_mnist.RData")
xy_axis <- data.frame(x = expand.grid(1:28, 28:1)[, 1],
                      y = expand.grid(1:28, 28:1)[, 2])
plot_theme <- list(
  raster = geom_raster(hjust = 0, vjust = 0),
  gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = FALSE),
  theme = theme_void()
)

sample_plots <- sample(1:nrow(fashion_mnist_train_X), 100) %>% map(~ {
  plot_data <- cbind(xy_axis, fill = data.frame(fill = fashion_mnist_train_X[.x, ]))
  ggplot(plot_data, aes(x, y, fill = fill)) + plot_theme
})

do.call("grid.arrange", c(sample_plots, ncol = 10, nrow = 10))
```

Beside l1/l2 regularization we can also use dropout. In this task you will build multi layer MLP with dropout regularization:

```{r fashion_mnist_model}
# TASK: Create MLP for fashion MNIST classification.
# Change labels vectors to one-hot-encoding matrix using to_categorical() function

# Scale pixel values to [0, 1] interval

# Model architecture:
# Dense layer with 512 units and "relu" activation
# Dropout layer with 20% drop rate
# Dense layer with 512 units and "relu" activation
# Dropout layer with 20% drop rate
# Output dense layer (how many units and what activation should You use?)

# Set SGD as optimizer and use categorical crossentropy as loss function. Use accuracy as additional metric.

# Fit the model. Use 20% of the data for validation, 20 epochs and 128 samples for batch size.

# Evaluate model on test set.
```

Training of a neural network can take a lot of time, solving a real life problem can take days, weeks, even months. During this time a lot of things can go wrong, for example if your machine will reset for some unknown reason you will lose all of your progress and a lot of time! To overcome this problem we can add model checkpoint that will save model on every epoch. Model checkpoint is one on many callbacks you cane use in Keras during training process:

```{r model_checkpoint}
# Create a new model for binaty classification and compile it.

# Fit the model and use `callbacks` argument with `model_checkpoint`
model_checkpoint <- callback_model_checkpoint(
  filepath = "models/bin_model.{epoch:02d}-{val_loss:.2f}.hdf5",
  monitor = "val_loss", # Monitored quantity
  period = 1) # Saving every 1 epoch
```

That's great, we can now resume training anytime. Callbacks can help us also with overfitting and long computation time. As you have seen in previous examples, some models tends to overfit if the number of epochs is too large. It would be great to let Keras know that we would like to stop training our model for example if validation accuracy didn't increased in a few epochs. This method is called early stopping and in Keras we can create a callback for this task:

```{r early_stopping}
# Create a new model for binary classification and compile it.

# Fit the model and use `callbacks` argument with:
# Create model checkpoint that will save best model into "models/bin_model.best.hdf5". Use validation accuracy as monitored quantity. Set `save_best_only` argument to TRUE to save only the model with best weights.
# Create early stopping callback using `callback_early_stopping` function. Set the same monitor as in model checkpoint and set `patience` to 3 epochs.

```

You know already that you can display your model architecture by simply printing your model in console, but sometimes it's convenient to display it as a graph.

```{r graph}
fashion_model %>% plot_model()
```
