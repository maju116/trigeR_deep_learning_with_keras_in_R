---
title: "part VI - Autoencoders"
author: "Micha≈Ç Maj"
output: html_notebook
---

```{r packages}
library(keras)
library(tidyverse)
library(ROCR)
```

Autoencoders can be used to many different things like image segmentation, dimensionality reduction, anomally detection and many more. Today we will start with the simple convolutional denoising autoencoder. We will use MNIST datatset avaliable in Keras:

```{r mnist}
mnist <- dataset_mnist()

mnist_train_X <- mnist$train$x / 255
mnist_test_X <- mnist$test$x / 255

mnist_train_X <- array_reshape(mnist_train_X, c(nrow(mnist_train_X), 28, 28, 1))
mnist_test_X <- array_reshape(mnist_test_X, c(nrow(mnist_test_X), 28, 28, 1))
```

In the denoising task we input to the autoencoder orginal image (data) with added noise. Output is simply orginal image (data). We want to minimise the **reconstruction error** between noised and orginal image. To do this we have to first create noised train dataset:

```{r noise_add_mnist}
clip_pixels <- function(tensor, min_value, max_value) {
  ifelse(tensor <= min_value,  min_value, ifelse(tensor >= max_value, max_value, tensor))
}

mnist_train_X_noise <- (mnist_train_X + rnorm(28 * 28 * 60000, 0, 0.5)) %>% clip_pixels(., 0, 1)
mnist_test_X_noise <- (mnist_test_X + rnorm(28 * 28 * 10000, 0, 0.5)) %>% clip_pixels(., 0, 1)
```

Now we can build simple denoising autoencoder:

```{r denoising_autoencoder_mnist}
# Ex. Create denosing autoencoder.
# 1. Model architecture:
# 2D convolution with 32 filters of size 3x3, 1x1 stride, 'relu' activation, "same" padding
# 2D max pooling size 2x2, 2x2 stride, "same" padding
# 2D convolution with 32 filters of size 3x3, 1x1 stride, 'relu' activation, "same" padding
# 2D max pooling size 2x2, 2x2 stride, "same" padding
# 2D convolution with 32 filters of size 3x3, 1x1 stride, 'relu' activation, "same" padding
# 2D upsampling layer of size 2x2
# 2D convolution with 32 filters of size 3x3, 1x1 stride, 'relu' activation, "same" padding
# 2D upsampling layer of size 2x2
# 2D convolution with 1 filters of size 3x3, 1x1 stride, 'sigmoid' activation, "same" padding
```

We will use `binary_crossentropy` as a loss function for pixel-wise comparision on input and output (you can think of it as a binary classification for each pixel).

```{r denoising_autoencoder_mnist_loss}
# 2. Compile the model
```

Now we can fit the model:

```{r denoising_autoencoder_mnist_fit}
# 3. Fit the model
```

Let's see predictions on the noised test set:

```{r denoising_autoencoder_mnist_predict}
autoencoder_predictions <- ____

plot_mnist <- function(im) image(1:28, 1:28, im, col = gray((0:255)/255))
par(mfcol = c(3, 6))
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i')
for (i in 7:12) {
  orginal <- t(apply(mnist_test_X[i,,,], 2, rev))
  noisy <- t(apply(mnist_test_X_noise[i,,,], 2, rev))
  reconstructed <- t(apply(autoencoder_predictions[i,,,], 2, rev))
  plot_mnist(orginal)
  plot_mnist(noisy)
  plot_mnist(reconstructed)
}
```

From example above you've seen taht in denosing task we were interested in **decoder** output (reconstruction), now we will use autoencoders for dimmensionality reduction. This time we will be interested in **encoder** output (low-dimmensional representation). Once again we will use MNIST dataset. This time we want to build MLP based autoencoder so we will have reshape the data into vectors:

```{r mnist_reshape}
# Ex. Create dim reduction autoencoder.
# 1. Reshape train/test set into 2d arrays (60000, 784)
mnist_train_X_vec <- ____
mnist_test_X_vec <- ____
```

This time we can not simply use sequential model, we will have to use a functional API:

```{r mnist_dr_autoencoder}
# 2. Create input layer with input_layer() function. Set correct shape parameter.
input <- ____

# 3. Create encoder architecture. On top of input add:
# Dense layer with 128 neurons and 'relu' activation
# Dense layer with 64 neurons and 'relu' activation
# Dense layer with 32 neurons and 'relu' activation
encoder <- ____

# 4. Create decoder architecture. On top of encoder add:
# Dense layer with 64 neurons and 'relu' activation
# Dense layer with 128 neurons and 'relu' activation
# Dense layer with 784 neurons and 'relu' activation
decoder <- ____

# 5. Create autoencoder model using functional API - keras_model() function
autoencoder <- keras_model(input, decoder)
# 6. Create encoder model using functional API - keras_model() function
encoder_model <- keras_model(input, encoder)
```

We will compile model with `mse` as loss function:

```{r mnist_dr_autoencoder_compile}
# 7. Compile autoencoder
```

Now we can fit the model. Remeber that this time input and output is the same!

```{r mnist_dr_autoencoder_fit}
# 8. Fit autoencoder
```

Now we can check reconstructions and low-dimmensional representations:

```{r mnist_dr_autoencoder_predict}
autoencoder_predictions <- autoencoder %>% predict(mnist_test_X_vec)
encoder_predictions <- encoder_model %>% predict(mnist_test_X_vec)

plot_mnist <- function(im, x, y) image(1:x, 1:y, im, col = gray((0:255)/255))
par(mfcol = c(3, 6))
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i')
for (i in 7:12) {
  orginal <- t(apply(matrix(mnist_test_X_vec[i,], 28, 28, byrow = TRUE), 2, rev))
  low_dim <- matrix(encoder_predictions[i,], 2, 16) %>% `/`(max(.))
  reconstructed <- t(apply(matrix(autoencoder_predictions[i,], 28, 28, byrow = TRUE), 2, rev))
  plot_mnist(orginal, 28, 28)
  plot_mnist(low_dim, 2, 16)
  plot_mnist(reconstructed, 28, 28)
}
```

In the final task we will use autoencoders for anomaly detection. We will use credictard fraud dataset:

```{r creditcard}
load("data/creditcard.RData")
table(creditcard_train_Y)
```

In the anomaly detection task we are interested in finding samples (transactions) that have high reconstruction error, which can tell us that there is something out of ordinary in there. In this task we can use sequential model similarly like in denosing task.

```{r creditcard_am_autoencoder}
# Ex. Create anomaly detection autoencoder
# 1. Architecture. Start with sequential model and add:
# Dense layer with 14 neurons and 'tanh' activation and l1 activity regularization with lambda = 10e-5
# Dense layer with 7 neurons and 'relu' activation
# Dense layer with 7 neurons and 'tanh' activation
# Dense layer with 29 neurons and 'relu' activation
autoencoder <- ____
```

We will use `mse` as loss:

```{r creditcard_am_autoencoder_compile}
# 2. compile the model
```

and now we can fit the model:

```{r creditcard_am_autoencoder_fit}
# 3. Fit the model
```

Now we can calculate reconstruction error for test set and find the best cutoff for creditcard fraud:

```{r creditcard_am_autoencoder_cutoff}
predictions <- autoencoder %>% predict(creditcard_test_X)
reconstruction_error <- apply((creditcard_test_X - predictions)^2, 1, mean)
results <- tibble(reconstruction_error = reconstruction_error, fraud = creditcard_test_Y)

pred <- prediction(results$reconstruction_error, results$fraud)
f.scores <- performance(pred, "f", alpha = 0.0005)
best_cutoff <- f.scores@x.values[[1]][which.max(f.scores@y.values[[1]])]
table(results$fraud, results$reconstruction_error > best_cutoff)
ggplot(results, aes(reconstruction_error, fill = as.factor(fraud))) + geom_histogram(bins = 100) + theme_bw() + facet_grid(fraud ~ ., scales = "free") + geom_vline(xintercept = best_cutoff)
```
